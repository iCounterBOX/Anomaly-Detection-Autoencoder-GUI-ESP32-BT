# https://youtu.be/q_tpFGHiRgg
"""
260 - Identifying anomaly images using convolutional autoencoders
(Sorting an entire image as either normal or anomaly)

Here, we use both the reconstruction error and also the kernel density estimation
based on the vectors in the latent space. We will consider the bottleneck layer output
from our autoencoder as the latent space. 

This code uses the malarial data set but it can be easily applied to 
any application. 

Data from: https://lhncbc.nlm.nih.gov/LHC-publications/pubs/MalariaDatasets.html
https://www.youtube.com/watch?v=q_tpFGHiRgg&list=PLZsOBAyNTZwYE5IM1g_3yNXuxaH_QRSoJ&index=4

KRISTINA:
28.07.24: HIER soll jetzt NUR das Model gebaut werden welches DANN in der Anomaly-Detection
          angewendet wird

"""

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, UpSampling2D
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import tensorflow as tf
import keras

import matplotlib.pyplot as plt
import matplotlib.image as mpimg
import random



print( str(tf.__version__))
print( str(keras.__version__))

#Size of our input images
SIZE = 128  # original: 128

#############################################################################
#Define generators for training, validation and also anomaly data.

batch_size = 64
datagen = ImageDataGenerator(rescale=1./255)

train_generator = datagen.flow_from_directory(
    'trainImages/errorFree_train/',
    target_size=(SIZE, SIZE),
    batch_size=batch_size,
    class_mode='input'   # ,color_mode='grayscale'
    )

validation_generator = datagen.flow_from_directory(
    'trainImages/errorFree_test/',
    target_size=(SIZE, SIZE),
    batch_size=batch_size,
    class_mode='input'  # ,color_mode='grayscale'   
    )

anomaly_generator = datagen.flow_from_directory(
    'trainImages/defect/',
    target_size=(SIZE, SIZE),
    batch_size=batch_size,
    class_mode='input'   # ,color_mode='grayscale'   
    )

#Define the A U T O E N C O D E R. 
#Try to make the bottleneck layer size as small as possible to make it easy for
#density calculations and also picking appropriate thresholds. 
# Strides? 

#Encoder
model = Sequential()
model.add(Conv2D(64, (3, 3), activation='relu', padding='same', input_shape=(SIZE, SIZE, 3)))  # original: (SIZE, SIZE, 3
model.add(MaxPooling2D((2, 2), padding='same'))   ## strides?
model.add(Conv2D(32, (3, 3), activation='relu', padding='same'))
model.add(MaxPooling2D((2, 2), padding='same'))
model.add(Conv2D(16, (3, 3), activation='relu', padding='same'))
model.add(MaxPooling2D((2, 2), padding='same'))

#Decoder
model.add(Conv2D(16, (3, 3), activation='relu', padding='same'))
model.add(UpSampling2D((2, 2)))
model.add(Conv2D(32, (3, 3), activation='relu', padding='same'))
model.add(UpSampling2D((2, 2)))
model.add(Conv2D(64, (3, 3), activation='relu', padding='same'))
model.add(UpSampling2D((2, 2)))

model.add(Conv2D(3, (3, 3), activation='sigmoid', padding='same'))  # original (3, (3, 3), a

model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mse'])
model.summary()

#Fit the model. 
history = model.fit(
        train_generator,
        steps_per_epoch= 500 // batch_size,
        epochs=1000,  # er hatte 1000
        validation_data=validation_generator,
        validation_steps=75 // batch_size,
        shuffle = True)

#.... DAS ..kann ...dauern!!   Z e i t f r e s s e r  
# write this TRAINRD model to folder  -  e n c o d e r D e c o d e r _ m o d e l 

model.save('model_encodeDecode.h5')


# ab hier ist nur noch  T E S T    und validation
#plot the training and validation accuracy and loss at each epoch - LOSS CURVE
loss = history.history['loss']
val_loss = history.history['val_loss']
epochs = range(1, len(loss) + 1)
plt.plot(epochs, loss, 'y', label='Training loss')
plt.plot(epochs, val_loss, 'r', label='Validation loss')
plt.title('Training and validation loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()

'''
PREDICTION-Test     model.EVALUATE
model.evaluate() function will give you the LOSS value for every batch.  
let us examine the reconstruction error between our validation data (good/normal images)
and the anomaly images  
'''
validation_error = model.evaluate(validation_generator) # Model.evaluate_generator` is deprecated 
anomaly_error = model.evaluate(anomaly_generator)
print("accuracy / Recon. error for the validation (normal) data is: ", validation_error)
print("accuracy /Recon. error for the anomaly data is: ", anomaly_error)


'''
PREDICTION-Test     model.predict
Get all batches generated by the datagen and pick a batch for prediction
Just to test the model. 1 Batch has 64 images 
'''

data_batch = []  #Capture all training batches as a numpy array
img_num = 0
while img_num <= train_generator.batch_index:   #gets each generated batch of size batch_size
    data = train_generator.next()
    data_batch.append(data[0])
    img_num = img_num + 1

predicted = model.predict(data_batch[0])  #Predict on the first batch of 64 images

#Sanity check, view few images and corresponding reconstructions -  Model Test simple image recognition (self ) 
image_number = random.randint(0, predicted.shape[0]) # max 64
plt.figure(figsize=(12, 6))
plt.subplot(121)
plt.imshow(data_batch[0][image_number])
plt.subplot(122)
plt.imshow(predicted[image_number])
plt.show()
mpimg.imsave("capFrame/myPredictImg.png", predicted[image_number] )   

# m a k e   t h e   a n o m a l y   m o d e l 


#Let us extract (or build) the encoder network, with trained weights.
#This is used to get the compressed output (latent space) of the input image. 
#The compressed output is then used to calculate the KDE
# DIESES neue Model bekommt die WEIGHTS des 1. model - braucht SO nicht nochmal trainiert werden

encoder_model = Sequential()
encoder_model.add(Conv2D(64, (3, 3), activation='relu', padding='same', input_shape=(SIZE, SIZE, 3), weights=model.layers[0].get_weights()) )
encoder_model.add(MaxPooling2D((2, 2), padding='same'))
encoder_model.add(Conv2D(32, (3, 3), activation='relu', padding='same', weights=model.layers[2].get_weights()))
encoder_model.add(MaxPooling2D((2, 2), padding='same'))
encoder_model.add(Conv2D(16, (3, 3), activation='relu', padding='same', weights=model.layers[4].get_weights()))
encoder_model.add(MaxPooling2D((2, 2), padding='same'))

#only can save if compiled
encoder_model.compile(optimizer='adam', loss='binary_crossentropy')
encoder_model.summary()

# write this model to folder -  T H E   A N O M A L Y _ M O D E L 

encoder_model.save('model_anomalyFinal.h5')


